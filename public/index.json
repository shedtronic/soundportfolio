
[{"content":"","date":"14 February 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"14 February 2025","externalUrl":null,"permalink":"/","section":"Sound Portfolio","summary":"","title":"Sound Portfolio","type":"page"},{"content":"As part of my research into immersion within a 360 audio environment, I have been exploring FMOD within Unity but more recently weighing up whether to invest time into FMOD within Unreal. In more recent updates Unreal Engine 5 now has its own dedicated interactive sound engine, MetaSounds. I am currently exploring ways to incorporate this into a personal project. I am in the process of building a working demonstration of techniques. In the lead up to this point, I am going to test a range of applications of MetaSound blueprints and explore some field recording techniques to capture the sounds I need.\nOne such approach is to develop audio-led interactions in a demonstration of location-based sound triggers. So I am exploring at the moment a level designed in a forest which has sound elements that develop an atmosphere of fear within a third person/first person walking sim environment\nAs the purpose of this level is currently a demonstration of sound design, I wanted sufficient visuals to react to so I haven\u0026rsquo;t just gone with an early block out but created this within a forest environment which has some small shacks and houses to explore dotted around the forest.\nAs I progress with the sound design, I am going to be adding subtle background atmos effects first such as streams, footsteps and animal sounds to build the atmosphere, distant unnerving creature sounds and weather related sounds such as wind, rain and maybe the odd rumble of thunder.\nI have added a day night cycle where the day is shorter than the night in keeping with a northern hemisphere sub arctic circle ennvironment.\n","date":"14 February 2025","externalUrl":null,"permalink":"/posts/article1/","section":"Posts","summary":"","title":"Unreal MetaSound Exploration","type":"posts"},{"content":"","date":"10 February 2025","externalUrl":null,"permalink":"/projects/","section":"Projects","summary":"","title":"Projects","type":"projects"},{"content":"We Wish You Health is the first album by The Horses of the Gods, is an album that I feel embodies an ethos of resourceful production. We took a long time to record this almost entirely acoustic album, and we crafted it with minimal equipment and fit it together narratively like a puzzle. It was mixed and mastered entirely on an old laptop. We think that that we achieved a sonic depth that has resonated with listeners. The album captures the raw, sparse and organic essence of The Horses of the Gods, blending traditional elements with an atmospheric, almost hauntologically-infused eerie sound steeped in folklore.\nOur most popular track, John Barleycorn, is steadily growing in reach across streaming platforms and finding an engaged audience. We think its a testament to the effectiveness of the subtlety of the mixing and mastering process. Despite working with a pared-down setup, the focus on clarity, balance, and dynamic warmth ensured the final mix sounded rich and immersive.\nThis project was not only an exercise in DIY audio engineering but also a lesson in how the limitations of technology can shape a distinctive aesthetic. The final master was optimized for Spotify, Apple Music, and Bandcamp, ensuring that the album translated well across streaming platforms while maintaining its organic character.\nThe production and release of We Wish You Health is an example of how thoughtful mixing and mastering regardless of equipment can elevate a recording, proving that great sound is as much about technique and vision as it is about equipment. I definitely learned a lot about both mixing and mastering, and having since taken a short course in mastering realise how much I can do to improve the next album which is already in the early stages of recording.\n","date":"10 February 2025","externalUrl":null,"permalink":"/projects/project4/","section":"Projects","summary":"","title":"The Horses of the Gods - Debut Album","type":"projects"},{"content":"","date":"9 February 2025","externalUrl":null,"permalink":"/tags/acoustic/","section":"Tags","summary":"","title":"Acoustic","type":"tags"},{"content":"","date":"9 February 2025","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"9 February 2025","externalUrl":null,"permalink":"/tags/folk/","section":"Tags","summary":"","title":"Folk","type":"tags"},{"content":"","date":"9 February 2025","externalUrl":null,"permalink":"/categories/music/","section":"Categories","summary":"","title":"Music","type":"categories"},{"content":" Sow In - The Horses of the Gods - We Wish You Health # We Wish You Health by The Horses of the Gods Sow In was an early demo when putting together We Wish You Health. There is an early demo which was just me on an acoustic guitar but the first recording of this song was created at a village hall near Bath and essentially this track was build around that original recording. The atmosphere of the location was captured in this recording, as it was between two neolithic burial mounds, and on the edge of the area where the civil war battle of Lansdown took place. We have a tendency to pick remote or eerie locations for recording. It started as a simple zoom H4n recording in one take but the disadvantage was that the guitar and vocals were on the same track. We added an extra track of guitar at the time, but then set to a series of overdubs in the DAW to try to keep the village hall feel to it. I particularly like that there are some background atmos in this recording which you might hear such as a milkbottle top falling of the kitchen work top and some other faint atmos sounds in the background. The vocal harmonies and violin add a subtle accompaniment were added much later. The hardest thing to create here was the intimacy and the slow pace of the original demo whilst trying to make it sound more expansive and almost like a theme tune to some archived TV programme.\n","date":"9 February 2025","externalUrl":null,"permalink":"/posts/music1/","section":"Posts","summary":"","title":"Sow In - The Horses of the Gods","type":"posts"},{"content":"","date":"9 February 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" Introduction # I\u0026rsquo;ve been diving into the world of oscilloscope art using OsciRender and Blender, exploring how sound can directly influence visual animations. There\u0026rsquo;s something incredibly intriguing about how a simple waveform can create complex shapes and how those shapes can then be animated to follow a soundtrack or a new song.\nTo start, I\u0026rsquo;m working with sequences of simple 3D shapes, using Blender\u0026rsquo;s integration with OsciRender to transform sound into visual data. This allows me to experiment with how different frequencies and amplitudes affect the geometry of the visuals. By mapping sound elements to movement and transformations, I can create dynamic visuals that feel almost like they are being sculpted by the music itself.\nThe plan is to build up from these simpler forms to more abstract and experimental visuals. I want to push the boundaries of what is possible, blending traditional 3D animation with the organic unpredictability of sound-driven art. As I get more comfortable with the workflow, I\u0026rsquo;ll start introducing elements of glitch and distortion, aligning with the hauntological and liminal aesthetics that underpin much of my work.\nUltimately, this approach offers a new way to think about music videos and live performance visuals, where the sound doesn\u0026rsquo;t just accompany the visuals but is an integral part of how they are generated and perceived.\n","date":"7 February 2025","externalUrl":null,"permalink":"/posts/article3/","section":"Posts","summary":"","title":"OsciRender Sound Design with Blender","type":"posts"},{"content":"During the first lockdown, we collaborated with Fable Feast, a London-based immersive theatre company, to bring the poem Hatton\u0026rsquo;s Cross by Jim Jack to life. Working alongside actor Benjamin Peter Jones and a team of animators, we created a haunting animated piece.\nThe quiet streets of the lockdown era provided a unique backdrop for our field recordings, amplifying the sounds of nature that are often masked by constant traffic noise. This absence of urban noise pollution inspired us to explore how sound shapes our relationship with the environment. Armed with field recordings captured during eerie woodland explorations, we assembled layers of atmospheric audio in our DAW, experimenting with chilling sound effects to craft a spine-tingling soundscape.\nThe final production, Hatton\u0026rsquo;s Cross, is available on Fable Feast\u0026rsquo;s YouTube channel. For the full experience, dim the lights, turn up the volume, and immerse yourself in the eerie world we helped create.\n","date":"3 February 2025","externalUrl":null,"permalink":"/posts/soundtrack2/","section":"Posts","summary":"","title":"Fable Feast Cursed Verse Soundtrack Collab","type":"posts"},{"content":"","date":"3 February 2025","externalUrl":null,"permalink":"/tags/horror/","section":"Tags","summary":"","title":"Horror","type":"tags"},{"content":"","date":"3 February 2025","externalUrl":null,"permalink":"/categories/sound-design/","section":"Categories","summary":"","title":"Sound Design","type":"categories"},{"content":" Project Overview # I have been inspired for a long time by different visual programming systems. I got the bug when I discovered Max-Msp years ago and tinkered with that for a while before moving to Processing and then P5.js. So when I found out about Touch Designer it was like the best of both worlds, something that had a node based interface with the programming and interfacing capabilities of something link Processing. Its early days for me but as I am very interested in not only visuals and live projections for The Horses of the Gods, I am also looking at ways to integrate sound reactive visuals for performance. The project I am working on is to make reactive visuals to a soundtrack I have produced already but I think I can take this much further.\nThe potential for creating live, reactive visuals that respond to sound in real-time is both exciting and a bit of a learning curve. The idea of using OSC or MIDI to bridge the two platforms opens up a lot of possibilities, not just for performance but also for installations and experimental projects.\nTo challenge myself, I\u0026rsquo;m planning to set up a system where visuals react dynamically to audio generated in Ableton Live and eventually Unreal Engine. This will involve mapping MIDI or audio analysis data to elements in TouchDesigner, creating a feedback loop where sound influences visuals and vice versa. I also want to experiment with generating ambient sounds directly within TouchDesigner, using audio CHOPs, and blending those with more complex audio from my DAW.\nBy combining generative audio with reactive visuals, I hope to create something that feels alive - something that embodies the hauntological and liminal themes I am currently exploring. This process will likely involve a lot of experimentation, but I\u0026rsquo;m looking forward to seeing where it leads and what unexpected connections emerge along the way.\n","date":"18 January 2025","externalUrl":null,"permalink":"/projects/project8/","section":"Projects","summary":"","title":"Touch Designer Sound Design and Visual Interaction","type":"projects"},{"content":" Project Overview # I\u0026rsquo;m continuing my exploration of audio synthesis using the Arduino Uno and the Mozzi library, pushing the boundaries of what a simple microcontroller can achieve in sound design. My goal is to create a portable synthesis module that not only generates sound but also reacts to its environment in surprising and creative ways.\nThe initial experiment, where I connected three potentiometers on a breadboard, was just the start. It proved that the Arduino Uno, with minimal hardware, could produce responsive audio signals. The potentiometers offered real-time control over sound parameters, and this hands-on manipulation felt like an analog synth in its most stripped-down form.\nWith the Mozzi library, I\u0026rsquo;m now diving into more advanced audio synthesis techniques. The library\u0026rsquo;s flexibility opens up a world of possibilities, from generating waveforms to creating complex modulation patterns. I\u0026rsquo;m particularly excited about using Mozzi\u0026rsquo;s ability to process sensor input. This means I can connect environmental sensors - microphones, light sensors, or even motion detectors - and use the incoming data to shape the sound.\nThe idea of a sound module that reacts to its surroundings aligns perfectly with my fascination for blending natural, mechanical, and hidden information layers. Imagine a device that transforms the hum of a radiator, the rhythm of passing traffic, or the whisper of tree leaves into generative soundscapes. By integrating environmental sensory input, the module could create evolving audio that reflects the changing conditions around it.\nMoving forward, I\u0026rsquo;ll experiment with different sensors and refine the synthesis algorithms. The ultimate goal is to craft a portable, adaptive instrument that could be used in both sound design projects and live performances. I\u0026rsquo;m looking forward to sharing the next steps as I prototype this ambitious idea, combining the tactile immediacy of Arduino hardware with the sonic depth of the Mozzi library.\n","date":"15 January 2025","externalUrl":null,"permalink":"/projects/project2/","section":"Projects","summary":"","title":"Oscillating Patches: Arduino Mozzi Explorations","type":"projects"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/categories/arduino/","section":"Categories","summary":"","title":"Arduino","type":"categories"},{"content":" Introduction # Summary: This post explores the initial stages of synthesis development using an Arduino Uno and the Mozzi library. By connecting three potentiometers on a breadboard, I tested sound generation capabilities and laid the groundwork for portable synthesis modules. This experiment aims to merge environmental sensory input with generative sound synthesis, drawing inspiration from the layers of natural, mechanical, and hidden information present in our surroundings.\nContent:\nIn this experiment, I set up an Arduino Uno for synthesis development using a combination of stock components and found materials. My goal was to test the basic sound generation capabilities of the system and establish a foundation for future development.\nThe Setup\nUsing a breadboard, I connected three potentiometers to the Arduino. Each potentiometer served as a control for different aspects of sound generation, allowing for real-time manipulation. This basic setup demonstrated that the Arduino Uno could produce responsive audio signals with minimal hardware.\nNext Steps with the Mozzi Library\nWith this initial test complete, I\u0026rsquo;m delving deeper into the Mozzi library, a powerful tool for audio synthesis on microcontrollers. The library will allow for more complex sound design, enabling the creation of portable synthesis modules that can react to various stimuli.\nExploring Generative Soundscapes\nOne of my key interests is to explore how the built environment, with its layers of natural and mechanical sounds, can influence generative audio synthesis. By incorporating sensors to capture auditory, visual, and hidden data from the environment, I aim to create a sound-sensing module. This module will not only respond to ambient stimuli but also allow for intentional manipulation, resulting in unique generative soundscapes.\nThis experiment is the beginning of a journey into portable, adaptive synthesis. I look forward to sharing updates as I refine these ideas and integrate more sensory elements into the design.\n","date":"14 January 2025","externalUrl":null,"permalink":"/posts/article6/","section":"Posts","summary":"","title":"Arduino Mozzi Synth Progress","type":"posts"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/mozzi/","section":"Tags","summary":"","title":"Mozzi","type":"tags"},{"content":"","date":"14 January 2025","externalUrl":null,"permalink":"/tags/synth/","section":"Tags","summary":"","title":"Synth","type":"tags"},{"content":" Project Overview # This project is going to be an exploration of the immersive potential of MetaSounds within Unreal Engine 5, focusing on how dynamic audio can enhance the atmosphere of a game environment. As part of my research into 360-degree audio environments, I initially experimented with FMOD in Unity but have since shifted my focus to Unreal Engine\u0026rsquo;s native sound engine, MetaSounds. This change is driven by the robust, integrated nature of MetaSounds, allowing for seamless audio interaction within the engine.\nGoals and Objectives\nThe primary goal of this project is to build a working demonstration of audio-driven interactions using MetaSounds. Specifically, I am developing a forest-based level designed to create an evolving atmosphere of fear. The level is a blend of third-person and first-person exploration, where sound plays a critical role in guiding and unsettling the player.\nThe objective is not only to demonstrate my sound design skills but also to explore how audio cues can drive player behavior and reinforce the narrative. By using location-based sound triggers, I aim to create an experience where the soundscape adapts to the player\u0026rsquo;s movements and decisions.\nThe Environment: Setting the Scene\nTo make the audio elements as effective as possible, I decided to build a semi-realistic forest environment rather than rely on a basic block-out. The level features: -\tDense woodland with layered vegetation for natural occlusion of sounds. -\tScattered cabins and shacks, each with its own unique audio triggers. -\tA day-night cycle where the night is longer than the day, reinforcing a sense of unease.\nThe forest setting is perfect for experimenting with spatial audio and dynamic soundscapes. Sounds such as rustling leaves, creaking wood, and distant animal calls are designed to immerse the player and add a subtle but persistent tension.\nSound Design Approach\nInitial Atmospherics\nMy first task is to lay down a subtle but rich ambient soundscape. Using MetaSounds, I am implementing: -\tBackground atmospherics like wind rustling through trees and distant streams. -\tEnvironmental sounds such as animal noises, footsteps, and water sounds. -\tWeather elements including dynamic rain and thunder, which can change based on the player\u0026rsquo;s location and in-game events.\nThese sounds are all generated or manipulated through MetaSound blueprints, allowing them to respond dynamically to in-game triggers.\nBuilding Fear Through Sound\nAs I develop the sound design, my focus will shift from the ambient and natural to the unsettling and supernatural. The plan includes: -\tDynamic creature sounds that react to the player\u0026rsquo;s proximity and actions. -\tAudio cues for interactive objects, such as creaking doors or whispers near certain items. -\tProcedural music layers that can adapt to the player\u0026rsquo;s choices, intensifying during key moments of the game.\nUsing MetaSounds\u0026rsquo; procedural audio tools, I can create evolving soundscapes that react to the game state and player input. This flexibility is crucial for maintaining a sense of unpredictability and danger.\nField Recording and Asset Creation\nTo create authentic and original sounds, I am combining: -\tField recordings of natural environments, including streams, wind, and wildlife. -\tFoley work to generate specific interactive sounds, such as footsteps on different surfaces. -\tSynthetic sound design using modular synthesis to create eerie and otherworldly audio elements.\nThese sounds will be layered and processed within MetaSounds to add depth and realism to the soundscape.\nNext Steps\nMy immediate focus is on testing and refining audio blueprints within Unreal Engine. So that I can: 1.\tPrototype audio interactions with basic objects in the level. 2.\tTest location-based audio triggers, ensuring they are immersive and well-balanced. 3.\tRefine the day-night audio cycle, adding subtle changes to the soundscape as time progresses.\nAs the project evolves, I will document my findings and challenges here, sharing insights into how MetaSounds can be used to enhance storytelling and player engagement through sound.\nThis project is a significant step towards building a portfolio of interactive audio design work, demonstrating not only technical skill but also a creative approach to sound as a narrative tool.\n","date":"10 January 2025","externalUrl":null,"permalink":"/projects/project1/","section":"Projects","summary":"","title":"Uncanny Game Sound: MetaSound In Unreal Engine ","type":"projects"},{"content":" Project Overview # This project is exploring the relationship between oscilloscope art and sound design. I am interested in ways to manipulate visuals using oscilloscope art using a program called OsciRender and sound. I eventually want to experiment with interactions using OsciRender, OSC protocol to allow ways to send sound from the DAW to Osci Render to allow audiences to participate with visuals but that will be explored once I have the connections I need. So I am going to begin with just manipulating OsciRender with Blender as they have good integration and you can bring in 3D animation into the artwork, but later I want to hook up Touch Designer to to this project to allow for more creative control, allowing introduction of glitch visuals and sound through OSC from Logic or Ableton.\nOscilloscope art has a distinct aesthetic that blends retro-futurism, glitch aesthetics, and scientific visualisation. The idea of using sound as a direct manipulator of shape and motion is fascinating - sound literally becoming the visual element.\nPhase 1: Exploring OsciRender in Blender\nTo start, I will focus on manipulating OsciRender within Blender, since the two integrate well. Blender\u0026rsquo;s animation and shading systems allow for 3D movement and distortions, adding another dimension to the traditionally 2D oscilloscope visuals. This phase will focus on: .\tLearning OsciRender\u0026rsquo;s waveform rendering techniques .\tCreating simple animated shapes using Blender\u0026rsquo;s modifiers .\tExperimenting with sound-generated waveforms to drive movement\nPhase 2: Expanding to TouchDesigner and OSC\nOnce I have a solid foundation in OsciRender and Blender, I will begin experimenting with TouchDesigner, an environment that excels at real-time visual manipulation. The goal is to integrate OSC (Open Sound Control) into the workflow, allowing dynamic interaction between sound and visuals in real time. Potential avenues for exploration include: .\tUsing OSC to send data from a DAW (Logic Pro/Ableton) to TouchDesigner .\tIntroducing glitch aesthetics and generative visual distortions .\tDeveloping interactive elements where an audience can influence the visuals through sound\n","date":"31 October 2024","externalUrl":null,"permalink":"/projects/project6/","section":"Projects","summary":"","title":"Oscilloscope Harmonics: Sound Design through Blender and OSC","type":"projects"},{"content":" Project Overview # This project explores the use of Sonic Pi as a tool for creating generative soundscapes and live-coded music. Sonic Pi offers a unique opportunity to combine coding with sound design, allowing me to build dynamic audio environments that can evolve in real-time. My goal is to develop a series of experiments using Sonic Pi to create ambient and rhythmic compositions, ultimately leading to live performance opportunities or integration with other interactive media projects.\nGoals and Objectives\nThe main objective of this project is to harness Sonic Pi\u0026rsquo;s capabilities for: -\tGenerative music creation, where patterns and sequences are generated algorithmically. -\tLive coding performance, experimenting with how sound can be manipulated on the fly. -\tIntegration with external hardware such as MIDI controllers or modular synths.\nA key focus is on creating soundscapes that explore the themes of hauntology, liminality, and the eerie concepts that align with my broader sound design practice.\nThe Setup: Tools and Techniques\nI am using: -\tSonic Pi software, running on a Mac with a MIDI keyboard for real-time input. -\tCode-based sequencing to generate evolving patterns and textures. -\tSynth and sample manipulation within Sonic Pi, blending organic and synthetic sounds.\nThe initial experiments involve building simple loops and patterns, then gradually introducing randomization and conditional logic to create a sense of unpredictability.\nGenerative Sound Design Approach\nInitial Experiments\nMy first experiments focus on: -\tAlgorithmic rhythms, using random seeds to vary percussion sequences. -\tEvolving melodies, with generative note selection based on predefined scales. -\tAmbient textures, using reverb and delay to create a sense of space and depth.\nTaking It Further: Integration and Performance\nThe next phase involves: -\tConnecting Sonic Pi to external hardware, such as a MIDI controller for live manipulation of parameters. -\tExploring OSC (Open Sound Control) integration, potentially linking Sonic Pi to visual applications like TouchDesigner or OsciRender. -\tBuilding sound-driven visuals, using Sonic Pi as an audio source for reactive animation.\nA key experiment will be to create a live-coded performance where visuals and audio are tightly synchronized, allowing audiences to experience a fully immersive and dynamic show.\nThematic Exploration\nThis project also offers a chance to explore how generative sound can convey specific themes: -\tHauntological elements, using delay and echo to create ghostly repetitions. -\tUrban Wyrd vibes, by layering field recordings with synthesized drones. -\tExploring liminal spaces, using generative audio to represent places that exist on the edge of the known and the unknown.\nBy experimenting with how different parameters affect the sound output, I aim to develop a workflow that allows for spontaneous yet controlled musical expression.\nNext Steps\nIn the coming weeks, I will: 1.\tRefine my Sonic Pi codebase, building a library of reusable functions and patterns. 2.\tTest integration with external visuals, exploring how sound can drive animations. 3.\tRecord a series of short audio experiments, possibly releasing them as a mini album or sound pack.\nI will document this process through blog posts and audio clips, sharing insights into how coding can be used as a creative tool for sound design.\nUltimately, this project is about pushing the boundaries of generative audio, exploring how coding can bring new life to sound design and performance.\n","date":"31 October 2022","externalUrl":null,"permalink":"/projects/project7/","section":"Projects","summary":"","title":"Generative Sonic Pi Sound Experiments","type":"projects"},{"content":" Project Overview # ","date":"3 June 2020","externalUrl":null,"permalink":"/projects/project3/","section":"Projects","summary":"","title":"Eerie Narratives: The Fable Feast Soundtrack","type":"projects"},{"content":"","date":"14 March 2020","externalUrl":null,"permalink":"/tags/atari-punk-console/","section":"Tags","summary":"","title":"Atari Punk Console","type":"tags"},{"content":" Introduction # Atari Punk Console Light Oscillator # The Atari Punk Console Light Oscillator is a creative DIY project that merges sound synthesis with light modulation. This project explores how light interacts with sound waves, generating unique and dynamic tones.\nFeatures: # Classic Atari Punk Console Circuit: Based on the 555 timer IC. Light Modulation: Using photoresistors to alter sound frequencies in real time. Interactive Controls: Adjustable knobs and light sources for custom soundscapes. Video Demo: # Build Instructions: # Gather Components: 556 timer IC Photoresistors Potentiometers Capacitors and resistors Speaker or audio output Assemble the Circuit: Follow the classic Atari Punk Console schematic and integrate the light modulation section. Test and Modify: Experiment with different light sources and adjust the controls to explore various sound possibilities. Applications: # This project is perfect for:\nBeginners in electronics and sound design. Exploring sound modulation with environmental inputs. Creating unique soundscapes for artistic or musical projects. Resources: # Full Schematic Code Repository (if applicable) Learn More About the Atari Punk Console ","date":"14 March 2020","externalUrl":null,"permalink":"/posts/article8/","section":"Posts","summary":"","title":"Atari Punk Console/Light Oscillator","type":"posts"},{"content":"","date":"14 March 2020","externalUrl":null,"permalink":"/categories/circuits/","section":"Categories","summary":"","title":"Circuits","type":"categories"},{"content":" Project Overview # Summary: This project explores the creation of an Atari Punk Console (APC) with a light-sensitive oscillator. By integrating light-dependent resistors (LDRs) into the classic APC design, this experiment investigates how light variations can influence sound synthesis. The project combines DIY electronics with creative sound design, showcasing the potential for interactive, light-driven sonic outputs.\nBackground The Atari Punk Console is a classic DIY sound synthesis circuit known for its raw, lo-fi sound. Based on the 556 dual-timer IC, it produces square wave tones with a characteristic retro, arcade-like aesthetic. By modifying the standard APC design to include light-sensitive components, the project aims to add a layer of environmental interactivity to the synthesis process.\nKey Features of the Project 1.\tLight Sensitivity .\tLight-Dependent Resistors (LDRs): Two LDRs are wired into the circuit to modulate the oscillators based on ambient light levels. .\tReal-Time Interaction: Changes in light intensity directly influence pitch and tone, creating a dynamic and interactive sound experience. 2.\tCircuit Design .\tCore Components: A 556 dual-timer IC forms the backbone of the APC, with potentiometers controlling key parameters like pitch and tone. .\tModifications: LDRs replace or supplement standard potentiometer controls, allowing for light-reactive modulation. 3.\tSound Output .\tProduces noisy, glitchy, and tonal variations influenced by light input. .\tOutput can be fed into additional effects or amplified for standalone use.\nChallenges and Insights Calibration involved adjusting the LDR sensitivity to achieve a desirable range of pitch modulation. Testing in various lighting conditions emphasized the need to design for both controlled and uncontrolled environments.\nApplications and Future Plans The APC light oscillator could serve as an expressive instrument for live performances, reacting to stage lighting or handheld torches. It has potential as a tool for generative soundscapes when combined with other modular synth elements. The project’s straightforward design also makes it suitable for teaching basic electronics and sound synthesis concepts.\nMaterials Used .\t556 Timer IC .\tLight-Dependent Resistors (LDRs) .\tPotentiometers (for additional manual control) .\tCapacitors and Resistors .\tBreadboard and Jumper Wires .\tPower Source: 9V battery or equivalent\nProgress So Far This project has successfully integrated LDRs into the APC design, demonstrating the potential for light-responsive sound synthesis. The results are promising, with rich sonic textures emerging from light interactions. Next steps include creating a portable enclosure, experimenting with LED feedback loops for added visual elements, and integrating the APC with other modular or DIY synth projects for expanded functionality.\nThis project is a step toward blending retro-inspired sound synthesis with modern, interactive technologies. It represents a fusion of nostalgia, experimentation, and creative expression, aligning with the broader goals of my sound design practice.\n","date":"31 May 2017","externalUrl":null,"permalink":"/projects/project5/","section":"Projects","summary":"","title":"The Atari Punk Console/Light Oscillator","type":"projects"},{"content":" Koochie - Regicide - The Ruler EP # undesignuk · Regicide - Koochie - Written by Mike Ballard 1995 ","date":"3 June 1995","externalUrl":null,"permalink":"/posts/music2/","section":"Posts","summary":"","title":"Koochie - Regicide","type":"posts"},{"content":"","date":"3 June 1995","externalUrl":null,"permalink":"/tags/pop/","section":"Tags","summary":"","title":"Pop","type":"tags"},{"content":"","date":"3 June 1995","externalUrl":null,"permalink":"/tags/punk/","section":"Tags","summary":"","title":"Punk","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]